{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> <a href=\"../README.md\">Vertex AI: Gemini Evaluations Playbook </a><br>\n",
    "Experiment, Evaluate, and Analyze</h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-gemini_evals_playbook_evaluate-from_notebook-colab&utm_medium=aRT-clicks&utm_campaign=gemini_evals_playbook_evaluate-from_notebook-colab&destination=gemini_evals_playbook_evaluate-from_notebook-colab&url=https%3A%2F%2Fcolab.sandbox.google.com%2Fgithub%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fblob%2Fmain%2Fgenai-on-vertex-ai%2Fgemini_evals_playbook%2Fnotebooks%2F1_gemini_evals_playbook_evaluate.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-gemini_evals_playbook_evaluate-from_notebook-colab_ent&utm_medium=aRT-clicks&utm_campaign=gemini_evals_playbook_evaluate-from_notebook-colab_ent&destination=gemini_evals_playbook_evaluate-from_notebook-colab_ent&url=https%3A%2F%2Fconsole.cloud.google.com%2Fvertex-ai%2Fcolab%2Fimport%2Fhttps%3A%252F%252Fraw.githubusercontent.com%252FGoogleCloudPlatform%252Fapplied-ai-engineering-samples%252Fmain%252Fgenai-on-vertex-ai%252Fgemini_evals_playbook%252Fnotebooks%252F1_gemini_evals_playbook_evaluate.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-gemini_evals_playbook_evaluate-from_notebook-github&utm_medium=aRT-clicks&utm_campaign=gemini_evals_playbook_evaluate-from_notebook-github&destination=gemini_evals_playbook_evaluate-from_notebook-github&url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fblob%2Fmain%2Fgenai-on-vertex-ai%2Fgemini_evals_playbook%2Fnotebooks%2F1_gemini_evals_playbook_evaluate.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-gemini_evals_playbook_evaluate-from_notebook-vai_workbench&utm_medium=aRT-clicks&utm_campaign=gemini_evals_playbook_evaluate-from_notebook-vai_workbench&destination=gemini_evals_playbook_evaluate-from_notebook-vai_workbench&url=https%3A%2F%2Fconsole.cloud.google.com%2Fvertex-ai%2Fworkbench%2Fdeploy-notebook%3Fdownload_url%3Dhttps%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fblob%2Fmain%2Fgenai-on-vertex-ai%2Fgemini_evals_playbook%2Fnotebooks%2F1_gemini_evals_playbook_evaluate.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals Playbook: Experiment, Evaluate & Analyze\n",
    "\n",
    "This notebook shows you how to define experiments, run evaluations to assess model performance, and analyze evaluation results including side-by-side comparison of results across different experiments and runs. The notebook performs following steps:\n",
    "\n",
    "- Define the evaluation task\n",
    "- Prepare evaluation dataset\n",
    "- Define an experiment by:\n",
    "    - Configuring the model\n",
    "    - Setting prompt and system instruction\n",
    "    - Establishing evaluation criteria (metrics)\n",
    "- Run evaluations using [Vertex AI Rapid Eval SDK](https://cloud.google.com/vertex-ai/generative-ai/docs/models/rapid-evaluation)\n",
    "- Log detailed results and summarizing through aggregated metrics.\n",
    "- Side-by-side comparison of evaluation runs for a comprehensive analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöß 0. Pre-requisites\n",
    "\n",
    "Make sure that you have prepared the environment following steps in [0_gemini_evals_playbook_setup.ipynb](0_gemini_evals_playbook_setup.ipynb). If the 0_gemini_evals_playbook_setup notebook has been run successfully, the following are set up:\n",
    "\n",
    "* GCP project and APIs to run the eval pipeline\n",
    "* All the required IAM permissions\n",
    "* Environment to run the notebooks\n",
    "* Bigquery datasets and tables to track evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read configurations\n",
    "\n",
    "The configuration saved previously in [0_gemini_evals_playbook_setup.ipynb](0_gemini_evals_playbook_setup.ipynb) will be used for initializing variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)\n",
    "print(f'module_path: {module_path}')\n",
    "\n",
    "# Import all the parameters\n",
    "from utils.config import PROJECT_ID, LOCATION, STAGING_BUCKET_URI\n",
    "from utils.evals_playbook import Evals, generate_uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, SafetySetting, HarmCategory, HarmBlockThreshold\n",
    "from vertexai.preview.evaluation import EvalTask, constants, make_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET_URI)\n",
    "\n",
    "print(\"Vertex AI SDK initialized.\")\n",
    "print(f\"Vertex AI SDK version = {vertexai.__version__}\")\n",
    "\n",
    "# pandas display full column values\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `Evals` object\n",
    "\n",
    "[`Evals`](../utils/evals_playbook.py) is a helper class helps to define tasks, experiments and log evaluation results. Define an instance of `Evals` class to use in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evals object\n",
    "evals = Evals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 1. Define and configure evaluation task and experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation Task\n",
    "\n",
    "An evaluation task defines the task model(s) will be evaluated on. The `task_id` is analogous to a workspace to group experiments and corresponding evaluation runs. This notebook premises on summarization of [PubMed](https://pubmed.ncbi.nlm.nih.gov/) articles as the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and log task\n",
    "task_id = \"task_summarization\"\n",
    "task = evals.Task(\n",
    "    task_id=task_id,\n",
    "    task_desc=\"summarize pubmed articles\",\n",
    "    tags=[\"pubmed\"]\n",
    ")\n",
    "evals.log_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List all tasks available in the database (lists tasks sorted by task creation time in descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals.get_all_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Experiment\n",
    "\n",
    "An experiment in Evals Playbook is defined by configuring\n",
    "- Dataset\n",
    "- Model and model configuration\n",
    "- Prompt\n",
    "\n",
    "Each experiment has an `experiment_id` and associated with a `task_id`. This sectio defines the required components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>‚ö†Ô∏è We recommend to create unique experiment id for each experiment to enable better tracking and experimentation. ‚ö†Ô∏è</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = \"Prompt with simple language summary\"\n",
    "# remove any special characters from experiment id\n",
    "_experiment_id = re.sub('[^0-9a-zA-Z]', '-', experiment_id.lower())\n",
    "experiment_desc = \"Update system instruction to generate a simple summary with bullets\"\n",
    "tags = [\"pubmed\"]\n",
    "metadata = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "#### Configure Model\n",
    "\n",
    "Define the Gemini model you want to evaluate your task on including name, configuration settings such as temperature and safety settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add [system instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions) to give the model additional context to understand the task, provide more customized responses, and adhere to specific guidelines over the full user interaction with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"Instruction: You are a medical researcher writing a plain language Summary of your Article for a layperson.\n",
    "\n",
    "Translate any medical terms to simple english explanations.\n",
    "Use first-person 'We'.  Use short bullet points addressing following\n",
    "- Purpose: What was the purpose of the study?\n",
    "- Research: What did the researchers do?\n",
    "- Findings: What did they find?\n",
    "- Implications: What does this mean for me?\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define generation config and safety settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"temperature\": 0.1,\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,threshold=HarmBlockThreshold.BLOCK_NONE),\n",
    "    SafetySetting(category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,threshold=HarmBlockThreshold.BLOCK_NONE),\n",
    "    SafetySetting(category=HarmCategory.HARM_CATEGORY_HARASSMENT,threshold=HarmBlockThreshold.BLOCK_NONE),\n",
    "    SafetySetting(category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,threshold=HarmBlockThreshold.BLOCK_NONE),\n",
    "]\n",
    "\n",
    "# safety_settings = {\n",
    "#     HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "#     HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "#     HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "#     HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro-001\" ,\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    "    system_instruction=system_instruction,\n",
    "    # TODO: Add tools and tool_config\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare a prompt template for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_id = \"short bulleted list with format\"\n",
    "prompt_description = \"instruction with short bullets addressing specific questions\"\n",
    "\n",
    "# Prompt Template\n",
    "prompt_template = \"Article: {context}. Summary:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configure prompt id, description for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = evals.Prompt(\n",
    "    prompt_id=prompt_id,\n",
    "    prompt_description=prompt_description,\n",
    "    prompt_type=\"single-turn\", # single-turn, chat,\n",
    "    is_multimodal=False,\n",
    "    system_instruction=system_instruction,\n",
    "    prompt_template=prompt_template,   \n",
    "    tags=tags\n",
    ")\n",
    "evals.log_prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare evaluation dataset\n",
    "\n",
    "This notebook uses a sample of [PubMed](https://pubmed.ncbi.nlm.nih.gov/) articles that are hosted on [HuggingFace](https://huggingface.co/datasets/ccdv/pubmed-summarization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download sample dataset (10 rows) of PubMed articles for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample dataset from PubMed articles\n",
    "ds_stream = load_dataset(\"ccdv/pubmed-summarization\", \"document\", split=\"test\", streaming=True)\n",
    "num_rows = 10\n",
    "dataset = Dataset.from_list(list(itertools.islice(ds_stream, num_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre-process and prepare dataset to use with the evaluator.\n",
    "\n",
    "Prepare the dataset as Pandas dataframe in the format expected by the [Vertex AI Rapid Eval SDK](https://cloud.google.com/vertex-ai/generative-ai/docs/models/rapid-evaluation#dataset-prep).\n",
    "\n",
    "Dataset column names:\n",
    "- `reference`: The column name of ground truth in the dataset.\n",
    "- `context`: The column name containing article passed as the context.\n",
    "- `instruction`: System instruction configured to pass to the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert HuggingFace dataset to Pandas dataframe\n",
    "eval_dataset = dataset.to_pandas()\n",
    "# rename columns as per Vertex AI Rapid Eval SDK defaults\n",
    "eval_dataset.columns = [\"context\", \"reference\"]\n",
    "# add instruction for calculating metrics (not all metrics need instruction)\n",
    "eval_dataset[\"instruction\"] = system_instruction\n",
    "# add prompt id for tracking\n",
    "eval_dataset[\"prompt_id\"] = [f\"prompt_{i}\" for i in eval_dataset.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Verify a few samples in the prepared evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optionally, save the dataset in Cloud Storage (or BigQuery) to reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = \"pubmed_summary.csv\" \n",
    "# eval_dataset_path = f'{STAGING_BUCKET_URI}/data/{_experiment_id}/{file_name}'\n",
    "# eval_dataset.to_csv(eval_dataset_path, index=False)\n",
    "# print(f\"Dataset saved to at {eval_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Metrics\n",
    "\n",
    "In this section, you configure the evaluation criteria for your task. You can choose from the [built-in metrics (or metric bundles)](https://cloud.google.com/vertex-ai/generative-ai/docs/models/rapid-evaluation#metric-bundles) from Vertex AI Rapid Eval SDK or define a custom metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define prebuilt/built-in metrics with Vertex GenAI Evaluation or bring your own metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    constants.Metric.ROUGE_1,\n",
    "    constants.Metric.ROUGE_L_SUM,\n",
    "    constants.Metric.BLEU,\n",
    "    constants.Metric.FLUENCY,\n",
    "    constants.Metric.COHERENCE,\n",
    "    constants.Metric.SAFETY,\n",
    "    constants.Metric.GROUNDEDNESS,\n",
    "    constants.Metric.SUMMARIZATION_QUALITY,\n",
    "    constants.Metric.SUMMARIZATION_VERBOSITY,\n",
    "    constants.Metric.SUMMARIZATION_HELPFULNESS\n",
    "]\n",
    "\n",
    "# build a metric config object for tracking\n",
    "metric_config = [\n",
    "    {\"metric_name\": metric, \"type\": \"prebuilt\", \"metric_scorer\": \"Vertex AI\"}\n",
    "    for metric in metrics\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Experiment\n",
    "\n",
    "Now that you have defined model, prompt, dataset and eval criteria (metrics), let's add them to an experiment and start logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = evals.log_experiment(task_id=task_id,\n",
    "                                  experiment_id=experiment_id,\n",
    "                                  experiment_desc=experiment_desc,\n",
    "                                  prompt=prompt,\n",
    "                                  model=model,\n",
    "                                  metric_config=metric_config,\n",
    "                                  tags=tags\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can view the experiment details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals.get_experiment(experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can view the prompt and system instruction if set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals.get_prompt(prompt_id=prompt_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List all experiments available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals.get_all_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 2. Run experiment(s) for an evaluation task\n",
    "\n",
    "The experiment is now ready to run an evaluation task using the model, prompt, dataset and metrics configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define [Vertex AI Rapid Eval Task](https://cloud.google.com/vertex-ai/generative-ai/docs/models/rapid-evaluation#evaluation-task). Evaluation tasks must contain an evaluation dataset, and a list of metrics to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_experiment_id = re.sub('[^0-9a-zA-Z]', '-', experiment_id.lower())\n",
    "eval_task = EvalTask(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    experiment=_experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the evaluation task with a run name, model and prompt template. This step may take a few minutes depending on the size of evaluation dataset.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>‚ö†Ô∏è A unique experiment run name is auto-generated based on experiment id. ‚ö†Ô∏è</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_run_name = generate_uuid(_experiment_id)\n",
    "eval_result = eval_task.evaluate(\n",
    "    model=model,\n",
    "    prompt_template=prompt_template,\n",
    "    experiment_run_name=experiment_run_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After the evaluation task is completed, Vertex AI Rapid Eval SDK returns the result of the  run including summary metrics and a detailed metrics table with per-instance (that is per example) metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_metrics = eval_result.summary_metrics\n",
    "report_df = eval_result.metrics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Log the run metrics (both summary and detail) to analyze or compare them in subsequent iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals.log_eval_run(experiment_run_id=experiment_run_name,\n",
    "                   experiment=experiment,\n",
    "                   eval_result=eval_result,\n",
    "                   tags=tags,\n",
    "                   metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View all evaluation runs for an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals.get_eval_runs(experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View all evaluation runs in the system across experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals.get_all_eval_runs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 3. Analyze results\n",
    "\n",
    "This section shows a few ways to analyze and compare results. Since the results are stored in BigQuery tables, there are multiple ways to analyze them\n",
    "\n",
    "1. Use BigQuery SQL queries\n",
    "2. Use Pandas dataframe and BigQuery\n",
    "3. Build Looker dashboards\n",
    "4. Use tools such as [LLM Comparator](https://medium.com/people-ai-research/llm-comparator-a-tool-for-human-driven-llm-evaluation-81292c17f521) from Google's PAIR team\n",
    "- and more ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get experiments, runs and run details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define `Evals` object to access helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = Evals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals.get_all_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get a specific experiment using `experiment_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals.get_experiment(experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic analysis\n",
    "\n",
    "#### Summary metrics\n",
    "\n",
    "Compare all runs for a given experiment at a summary level. This can be useful, when you run the same experiment at different time snapshots and allow you to see if there is any variance or change in eval metrics (how robust the model is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals.get_eval_runs(experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed metrics\n",
    "\n",
    "You can get a detail eval result for a given experiment run at example level. This helps you to analyze and identify any loss patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_run_id = \"\"\n",
    "evals.get_eval_run_detail(experiment_run_id=experiment_run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare eval runs across experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare eval runs at summary level\n",
    "\n",
    "You can compare summary metrics for multiple runs side-by-side even across different experiments. For example, you can compare eval runs \n",
    "- For the same prompt at different temperature settings\n",
    "- Same model setting but different prompt templates or system instruction\n",
    "\n",
    "\n",
    "Pass a list of experiment run ids and compare them side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ids = [\n",
    "    # list of run ids\n",
    "    ]\n",
    "evals.compare_eval_runs(run_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM Comparator for analyzing side-by-side LLM evaluation results\n",
    "\n",
    "To visualize model responses from different runs, we use [LLM Comparator](https://github.com/PAIR-code/llm-comparator) Python Library from [Google PAIR team](https://pair.withgoogle.com/) to compare model responses from two runs side-by-side. The tool coordinates the three phases of comparative evaluation: judging, bulletizing, and clustering and the results can be uploaded on [LLM Comparator app](https://pair-code.github.io/llm-comparator/) to view and analyze further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fetch run details for two experiment run ids you would like to compare. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Use <code>evals.get_all_eval_runs()</code> or <code>evals.get_eval_runs(experiment_id=experiment_id)</code> to get run ids.</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare run details to compare\n",
    "# @markdown ### Enter experiment run id 1\n",
    "run_1 = \"...\" # @param {type:\"string\"}\n",
    "run_1_details = evals.get_eval_run_detail(experiment_run_id=run_1)\n",
    "run_1_details = run_1_details[['run_id', 'example_id', 'input_prompt', 'output_text']]\n",
    "\n",
    "# @markdown ### Enter experiment run id 2\n",
    "run_2 = \"...\" # @param {type:\"string\"}\n",
    "run_2_details = evals.get_eval_run_detail(experiment_run_id=run_2)\n",
    "run_2_details = run_2_details[['run_id', 'example_id', 'input_prompt', 'output_text']]\n",
    "\n",
    "run1_run2 = pd.merge(run_1_details, run_2_details, how=\"outer\", on=[\"example_id\"], suffixes=(\"_1\", \"_2\"))\n",
    "run1_run2 = run1_run2.rename(columns={\"input_prompt_1\": \"prompt\", \n",
    "                                      \"output_text_1\": \"response_a\",\n",
    "                                      \"output_text_2\": \"response_b\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare pairwise comparison file to visualize using LLM Comparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_comparator import comparison\n",
    "from llm_comparator import model_helper\n",
    "from llm_comparator import llm_judge_runner\n",
    "from llm_comparator import rationale_bullet_generator\n",
    "from llm_comparator import rationale_cluster_generator\n",
    "\n",
    "inputs = run1_run2.to_dict(orient='records')\n",
    "\n",
    "custom_fields_schema = [\n",
    "    {'name': 'prompt_id', 'type': 'string'},\n",
    "]\n",
    "\n",
    "# Initialize the models-calling classes.\n",
    "generator = model_helper.VertexGenerationModelHelper(model_name='gemini-1.5-pro')\n",
    "embedder = model_helper.VertexEmbeddingModelHelper()\n",
    "\n",
    "# Initialize the instances that run work on the models.\n",
    "judge = llm_judge_runner.LLMJudgeRunner(generator)\n",
    "bulletizer = rationale_bullet_generator.RationaleBulletGenerator(generator)\n",
    "clusterer = rationale_cluster_generator.RationaleClusterGenerator(\n",
    "    generator, embedder\n",
    ")\n",
    "\n",
    "# Configure and run the comparative evaluation.\n",
    "comparison_result = comparison.run(inputs, judge, bulletizer, clusterer, judge_opts={\"num_repeats\": 2})\n",
    "\n",
    "# Write the results to a JSON file that can be loaded in\n",
    "# https://pair-code.github.io/llm-comparator\n",
    "file_path = \"assets/run1_run2_compare.json\"\n",
    "comparison.write(comparison_result, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can now upload this file on LLM Comparator tool/app at https://pair-code.github.io/llm-comparator/ and analyze the results. Refer to [documentation](https://github.com/PAIR-code/llm-comparator/tree/main?tab=readme-ov-file#using-llm-comparator) on how to use the tool.\n",
    "\n",
    "![LLM Comparator results](assets/llm_comparator_results.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis, you can identify loss patterns and seed idea for next experiment. For example, changing prompt template, system instruction or model configuration. [Add a new experiment](#Ô∏è-1-define-and-configure-evaluation-task-and-experiment) and run evaluations until you meet the success criteria for the evaluation task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleaning up\n",
    "\n",
    "Uncomment the following cells to clean up resources created as part of the Evals Playbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete BigQuery Dataset using bq utility\n",
    "# ! bq rm -r -f -d {BQ_DATASET_ID}\n",
    "\n",
    "# # Delete GCS bucket\n",
    "# ! gcloud storage rm --recursive {STAGING_BUCKET_URI}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "evals",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "evals",
   "language": "python",
   "name": "evals"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
